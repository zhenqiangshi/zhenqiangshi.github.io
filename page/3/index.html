<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"shizhenqiang.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
<meta property="og:type" content="website">
<meta property="og:title" content="Chelf Blog">
<meta property="og:url" content="https://shizhenqiang.github.io/page/3/index.html">
<meta property="og:site_name" content="Chelf Blog">
<meta property="og:description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
<meta property="og:locale">
<meta property="article:author" content="Chelf">
<meta property="article:tag" content="DO">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://shizhenqiang.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Chelf Blog</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chelf Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">To let your hair down</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chelf</p>
  <div class="site-description" itemprop="description">We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://shizhenqiang.github.io/2018/10/28/LogisticRegression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chelf">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chelf Blog">
      <meta itemprop="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Chelf Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/28/LogisticRegression/" class="post-title-link" itemprop="url">逻辑回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-28 15:24:14" itemprop="dateCreated datePublished" datetime="2018-10-28T15:24:14+08:00">2018-10-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-11-04 14:24:13" itemprop="dateModified" datetime="2022-11-04T14:24:13+08:00">2022-11-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="一、基本认识"><a href="#一、基本认识" class="headerlink" title="一、基本认识"></a>一、基本认识</h1><p>什么是逻辑回归？</p>
<p>面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。简单一句话，<strong>用回归的思想去解决分类问题。</strong></p>
<h1 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a>二、代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_load</span>():</span><br><span class="line">    X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(X,</span><br><span class="line">                                                        y,</span><br><span class="line">                                                        test_size=<span class="number">0.4</span>,</span><br><span class="line">                                                        random_state=<span class="number">15</span>,</span><br><span class="line">                                                        stratify=y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logic</span>():</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = data_load()</span><br><span class="line"></span><br><span class="line">    LR = LogisticRegression(C=<span class="number">1e3</span>, solver=<span class="string">&#x27;saga&#x27;</span>)</span><br><span class="line">    pca = PCA(n_components=<span class="number">2</span>, svd_solver=<span class="string">&quot;randomized&quot;</span>)</span><br><span class="line">    pca_x_train = pca.fit_transform(x_train)</span><br><span class="line">    LR.fit(pca_x_train, y_train)</span><br><span class="line">    plot_decision_regions(pca_x_train, y_train, clf=LR)</span><br><span class="line">    <span class="built_in">print</span>(LR.coef_.T)</span><br><span class="line">    <span class="built_in">print</span>(LR.intercept_)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    LR.fit(x_train, y_train)</span><br><span class="line">    <span class="comment"># 系数</span></span><br><span class="line">    <span class="built_in">print</span>(LR.coef_.T)</span><br><span class="line">    <span class="comment"># 截距</span></span><br><span class="line">    <span class="built_in">print</span>(LR.intercept_)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出预测的概率</span></span><br><span class="line">    <span class="built_in">print</span>(LR.predict_proba(x_test).<span class="built_in">round</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logic()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>代码解析，我们通过逻辑回归的模型得到了模型的系数和截距。得到了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X, copy=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the softmax function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The softmax function is calculated by</span></span><br><span class="line"><span class="string">    np.exp(X) / np.sum(np.exp(X), axis=1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This will cause overflow when large values are exponentiated.</span></span><br><span class="line"><span class="string">    Hence the largest value in each row is subtracted from each data</span></span><br><span class="line"><span class="string">    point to prevent this.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array-like of floats, shape (M, N)</span></span><br><span class="line"><span class="string">        Argument to the logistic function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    copy : bool, optional</span></span><br><span class="line"><span class="string">        Copy X or not.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    out : array, shape (M, N)</span></span><br><span class="line"><span class="string">        Softmax function evaluated at every point in x</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> copy:</span><br><span class="line">        X = np.copy(X)</span><br><span class="line">    max_prob = np.<span class="built_in">max</span>(X, axis=<span class="number">1</span>).reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    X -= max_prob</span><br><span class="line">    np.exp(X, X)</span><br><span class="line">    sum_prob = np.<span class="built_in">sum</span>(X, axis=<span class="number">1</span>).reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    X /= sum_prob</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<p>如上是一个softmax函数，那么得到的截距和系数，那么就可以得到预测结果, 下代码主要是解释一下，最终的预测结果是如何得出的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dd = np.dot(x_test, LR.coef_.T) + LR.intercept_</span><br><span class="line"><span class="built_in">print</span> (dd.ndim)</span><br><span class="line"><span class="built_in">print</span>(dd)</span><br><span class="line"><span class="comment"># predict </span></span><br><span class="line">predict_proda1 = pd.DataFrame(softmax(dd)).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># predict proba</span></span><br><span class="line">predict_proda2 = pd.DataFrame(LR.predict_proba(x_test).<span class="built_in">round</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#  predict_proda1 =  predict_proda2</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://shizhenqiang.github.io/2018/10/25/%E8%81%9A%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chelf">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chelf Blog">
      <meta itemprop="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Chelf Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/10/25/%E8%81%9A%E7%B1%BB/" class="post-title-link" itemprop="url">聚类综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-10-25 11:12:49" itemprop="dateCreated datePublished" datetime="2018-10-25T11:12:49+08:00">2018-10-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-25 11:28:21" itemprop="dateModified" datetime="2022-10-25T11:28:21+08:00">2022-10-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>​    所谓数据挖掘,就是从大量无序的数据中发现隐含的、有效的、有价值的、可理解的模式,进而发现有用的知识,并得出时间的趋向和关联,为用户提供问题求解层次的决策支持能力。</p>
<h2 id="二、-DM中的现有的聚类算法"><a href="#二、-DM中的现有的聚类算法" class="headerlink" title="二、 DM中的现有的聚类算法"></a>二、 DM中的现有的聚类算法</h2><p>   本文以聚类算法所采用的基本思想为依据将它们分为五类,即<strong>层次聚类算法</strong>、<strong>分割聚类算法</strong>、<strong>基于约束的聚类算法</strong>、<strong>机器学习中的聚类算法</strong>以及用于<strong>高维数据的聚类算法</strong>。</p>
<p>（1）层次聚类算法<br>         聚合聚类:Single2Link,Complete2Link,Average2Link分解聚类<br>         分割聚类算法基于密度的聚类基于网格的聚类<br>（2）基于图论的聚类<br>         基于平方误差的迭代重分配聚类:概率聚类、最近邻<br>         聚类、K2medoids、K2means<br>（3）基于约束的聚类算法<br>（4）机器学习中的聚类算法<br>         人工神经网络方法<br>         基于进化理论的方法:模拟退火、遗传算法<br>（5）用于高维数据的聚类算法<br>         子空间聚类<br>          联合聚类</p>
<h3 id="2-1-层次聚类算法"><a href="#2-1-层次聚类算法" class="headerlink" title="2.1 层次聚类算法"></a>2.1 层次聚类算法</h3><p>​    层次聚类算法通过将数据组织成若干组并形成一个相应的树状图来进行聚类,它又可以分为两类,即<strong>自底向上</strong>的聚合层次聚类和<strong>自顶向下</strong>的分解层次聚类。聚合聚类的策略是先将每个对象各自作为一个原子聚类,然后对这些原子聚类逐层进行聚合,直至满足一定的终止条件;后者则与前者相反,它先将所有的对象都看成一个聚类,然后将其不断分解直至满足终止条件。<br>​    对于聚合聚类算法来讲,根据度量两个子类的相似度时所依据的距离不同,又可将其分为基于Single-Link,Complete-Link和Average-Link的聚合聚类。Single-Link在这三者中应用最为广泛,它根据两个聚类中相隔最近的两个点之间的距离来评价这两个类之间的相似程度,而后两者则分别依据两类中数据点之间的最远距离和平均距离来进行相似度评价。<br>​    CURE,ROCK和CHAMELEON算法是聚合聚类中最具代表性的三个方法。<br>​    Guha等人在1998年提出了CURE算法。该方法不用单个中心或对象来代表一个聚类,而是选择数据空间中固定数目的、具有代表性的一些点共同来代表相应的类,这样就可以识别具有复杂形状和不同大小的聚类,从而能很好地过滤孤立点。ROCK算法[2]是对CURE的改进,除了具有CURE算法的一些优良特性之外,它还适用于类别属性的数据。CHAME-LEON算法是Karypis等人于1999年提出来的,它在聚合聚类的过程中利用了动态建模的技术。</p>
<h3 id="2-2-分割聚类算法"><a href="#2-2-分割聚类算法" class="headerlink" title="2.2 分割聚类算法"></a>2.2 分割聚类算法</h3><p>分割聚类算法是另外一种重要的聚类方法。它先将数据点集分为k个划分,然后从这k个初始划分开始,通过重复的控制策略使某个准则最优化以达到最终的结果。这类方法又可分为基于密度的聚类、基于网格的聚类、基于图论的聚类和基于平方误差的迭代重分配聚类。</p>
<h4 id="2-2-1-基于密度的聚类"><a href="#2-2-1-基于密度的聚类" class="headerlink" title="2.2.1 基于密度的聚类"></a>2.2.1 基于密度的聚类</h4><p>​    基于密度的聚类算法从数据对象的分布密度出发,将密度足够大的相邻区域连接起来,从而可以发现具有任意形状的聚类,并能有效处理异常数据。它主要用于对空间数据的聚类。<br><strong>DBSCAN是一个典型的基于密度的聚类方法</strong>,它将聚类定义为一组密度连接的点集,然后通过不断生长足够高密度的区域来进行聚类。DENCLUE[5]则根据数据点在属性空间中的密度来进行聚类。从本质上讲,DENCLUE是基于密度的聚类算法与基于网格的预处理的结合,它受目标数据的维度影响较小。此外,Ankerst等人提出的OPTICS,Xu等人提出的DB-CLASD和马帅等人提出的CURD算法也采用了基于密度的聚类思想,它们均针对数据在空间中呈现的不同密度分布对DB-SCAN作了相应的改进。</p>
<h4 id="2-2-2-基于网格的聚类"><a href="#2-2-2-基于网格的聚类" class="headerlink" title="2.2.2 基于网格的聚类"></a>2.2.2 基于网格的聚类</h4><p>​    基于网格的聚类从对数据空间划分的角度出发,利用属性空间的多维网格数据结构,将空间划分为有限数目的单元以构成一个可以进行聚类分析的网格结构。该方法的主要特点是处理时间与数据对象的数目无关,但与每维空间所划分的单元数相关;而且,基于其间接的处理步骤(数据→网格数据→空间划分→数据划分),该方法还与数据的输入顺序无关。与基于密度的聚类只能处理数值属性的数据所不同的是,基于网格的聚类可以处理任意类型的数据,但以降低聚类的质量和准确性为代价。STING是一个基于网格多分辨率的聚类方法,它将空间划分为方形单元,不同层次的方形单元对应不同层次的分辨率。STING+则对其进行了改进以用于处理动态进化的空间数据。CLIQUE也是一个基于网格的聚类算法,它结合了网格聚类与密度聚类的思想,对于处理大规模高维数据具有较好的效果。除这些算法以外,以信号处理思想为基础的Wave-Cluster算法也属基于网格聚类的范畴。</p>
<h4 id="2-2-3-基于图论的聚类"><a href="#2-2-3-基于图论的聚类" class="headerlink" title="2.2.3 基于图论的聚类"></a>2.2.3 基于图论的聚类</h4><p>​    基于图论的方法是把聚类转换为一个组合优化问题,并利用图论和相关的启发式算法来解决该问题。其做法一般是先构造数据集的最小生成树(MinimalSpanningTree,MST),然后逐步删除MST中具有最大长度的那些边,从而形成更多的聚类。基于超图的划分和基于光谱的图划分方法是这类算法的两个主要应用形式。该方法的一个优点在于它不需要进行一些相似度的计算,就能把聚类问题映射为图论中的一个组合优化问题。  2.2.4 基于平方误差的迭代重分配聚类<br>​    基于平方误差的重分配聚类方法的主要思想是逐步对聚类结果进行优化、不断将目标数据集向各个聚类中心进行重新分配以获得最优解(判断是否是最优解的目标函数通常通过平方误差计算法得到)。此类方法又可进一步分为概率聚类算法、考虑了最近邻影响的最近邻聚类算法以及K-medoids算法和K-means算法。<br>(1)概率聚类算法的重要代表是Mitchell等人于1997年<br>   提出的期望最大化算法(ExpectationMaximization,EM)[11]。它除了能处理异构数据之外,还具有另外几个重要的特性:</p>
<p>   ①能处理具有复杂结构的记录;</p>
<p>   ②能够连续处理成批的数据;</p>
<p>   ③具有在线处理能力;</p>
<p>   ④产生的聚类结果易于解释。<br>(2)最近邻距离的计算在聚类过程中起着基础性的作用,这也正是导致产生最近邻聚类算法的直接因素。共享最近邻算法(SharedNearestNeighbor,SNN)就是该类算法的典型代表之一,它把基于密度的方法与ROCK算法的思想结合起来,通过只保留数据点的K个最近邻居从而简化了相似矩阵,并且也保留了与每个数据点相连的最近邻居的个数,但是其时间复杂度也提高到了O(N2)(N为数据点个数)。<br>(3)K2medoids方法用类中的某个点来代表该聚类,这种方法能有效处理异常数据。它的两个最早版本是PAM和CLARA算法,此后又有CLARANS及其一系列的扩展算法。这类方法具有两个优点:它能处理任意类型的属性;它对异常数据不敏感。<br>(4)K2means算法是目前为止应用最为广泛的一种聚类方法,其每个类别均用该类中所有数据的平均值(或加权平均)来表示,这个平均值即被称作聚类中心。该方法虽然不能用于类别属性的数据,但对于数值属性的数据,它能很好地体现聚类在几何和统计学上的意义。 但是,原始K2means算法也存在如下缺陷:</p>
<p>​    ①聚类结果的好坏依赖于对初始聚类中心的选择;</p>
<p>​    ②容易陷入局部最优解;</p>
<p>​    ③对K值的选择没有准则可依循;</p>
<p>​    ④对异常数据较为敏感;</p>
<p>​    ⑤只能处理数值属性的数据;</p>
<p>​    ⑥聚类结果可能不平衡。<br>​    为克服原始K2means算法存在的不足,研究者从各自不同的角度提出了一系列K2means的变体,如Bradley和Fayyad等人从降低聚类结果对初始聚类中心的依赖程度入手对它作了改进,同时也使该算法能适用于大规模的数据集[15];Dhillon等人则通过调整迭代过程中重新计算聚类中心的方法使其性能得到了提高[16];Zhang等人利用权值对数据点进行软分配以调整其迭代优化过程[17];Pelleg等人提出了一个新的X2means算法来加速其迭代过程[18];Sarafis则将遗传算法应用于K2means的目标函数构建中,并提出了一个新的聚类算法[19];为了得到平衡的聚类结果,文献[20]利用图论的划分思想对K2means作了改进;文献[21]则将原始算法中的目标函数对应于一个各向同性的高斯混合模型;Berkhin等人[22]将K2means的应用扩展到了分布式聚类。</p>
<h3 id="2-3-基于约束的聚类算法"><a href="#2-3-基于约束的聚类算法" class="headerlink" title="2.3 基于约束的聚类算法"></a>2.3 基于约束的聚类算法</h3><p>​    真实世界中的聚类问题往往是具备多种约束条件的,然而由于在处理过程中不能准确表达相应的约束条件、不能很好地利用约束知识进行推理以及不能有效利用动态的约束条件,使得这一方法无法得到广泛的推广和应用。这里的约束可以是对个体对象的约束,也可以是对聚类参数的约束,它们均来自相关领域的经验知识。该方法的一个重要应用在于对存在障碍数据的二维空间数据进行聚类。COD(ClusteringwithOb-structedDistance)就是处理这类问题的典型算法,其主要思想是用两点之间的障碍距离取代了一般的欧氏距离来计算其间的最小距离。得这一方法无法得到广泛的推广和应用。这里的约束可以是对个体对象的约束,也可以是对聚类参数的约束,它们均来自相关领域的经验知识。该方法的一个重要应用在于对存在障碍数据的二维空间数据进行聚类。COD(ClusteringwithOb-structedDistance)就是处理这类问题的典型算法,其主要思想是用两点之间的障碍距离取代了一般的欧氏距离来计算其间的最小距离。更多关于这一聚类算法的总结可参考文献</p>
<h3 id="2-4-机器学习中的聚类算法"><a href="#2-4-机器学习中的聚类算法" class="headerlink" title="2.4 机器学习中的聚类算法"></a>2.4 机器学习中的聚类算法</h3><p>​    机器学习中的聚类算法是指与机器学习相关、采用了某些机器学习理论的聚类方法,它主要包括人工神经网络方法以及基于进化理论的方法。<br>​    自组织映射(Self2OrganizingMap,SOM)是利用人工神经网络进行聚类的较早尝试,它也是向量量化方法的典型代表之一。</p>
<p>​    该方法具有两个主要特点:</p>
<p>​      ①它是一种递增的方法,即所有的数据点是逐一进行处理的;</p>
<p>​      ②它能将聚类中心点映射到一个二维的平面上,从而实现可视化。</p>
<p>​     此外,文献[26]中提出的一种基于投影自适应谐振理论的人工神经网络聚类也具有很好的性能。在基于进化理论的聚类方法中,模拟退火的应用较为广泛,SINICC算法就是其中之一。在模拟退火中经常使用到微扰因子,其作用等同于把一个点从当前的聚类重新分配到一个随机选择的新类别中,这与K2means中采用的机制有些类似。遗传算法也可以用于聚类处理,它主要通过选择、交叉和变异这三种遗传算子的运算以不断优化可选方案从而得到最终的聚类结果。<br>​    利用进化理论进行聚类的缺陷在于它依赖于一些经验参数的选取,并且具有较高的计算复杂度。为了克服上述不足之处,有研究者尝试组合利用多种策略,如将遗传算法与K2means结合起来,并且使用变长基因编码,这样不仅能提高K2means算法的效率,还能运行多个K2means算法以确定合适的K值</p>
<h3 id="2-5-用于高维数据的聚类算法"><a href="#2-5-用于高维数据的聚类算法" class="headerlink" title="2.5 用于高维数据的聚类算法"></a>2.5 用于高维数据的聚类算法</h3><p>​    高维数据聚类是目前多媒体数据挖掘领域面临的重大挑战之一。对高维数据聚类的困难主要来源于以下两个因素:</p>
<p>​    ①高维属性空间中那些无关属性的出现使得数据失去了聚 类趋势;</p>
<p>​    ②高维使数据之间的区分界限变得模糊。</p>
<p>除了降维这一最直接的方法之外,对高维数据的聚类处理还包括子空间聚类以及联合聚类技术等。<br>CACTUS采用了子空间聚类的思想,它基于对原始空间在二维平面上的一个投影处理。CLIQUE也是用于数值属性数据的一个简单的子空间聚类方法,它不仅同时结合了基于密度和基于网格的聚类思想,还借鉴了Apriori算法,并利用MDL(MinimumDescriptionLength)原理选择合适的子空间。联合聚类对数据点和它们的属性同时进行聚类。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://shizhenqiang.github.io/2018/03/20/Random-Forests/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chelf">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chelf Blog">
      <meta itemprop="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Chelf Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/03/20/Random-Forests/" class="post-title-link" itemprop="url">Random Forests</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-03-20 15:44:26" itemprop="dateCreated datePublished" datetime="2018-03-20T15:44:26+08:00">2018-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-13 18:04:37" itemprop="dateModified" datetime="2022-10-13T18:04:37+08:00">2022-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p>
<p>​     其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，<em><strong>但在随机产生大量的决策树后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可能的分类。</strong></em></p>
<p><em><strong>决策树：</strong></em></p>
<p>​    决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p>
<p>   随机森林是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类，然后看看哪一类被选择最多，就预测这个样本为那一类。</p>
<p>​    在建立每一棵决策树的过程中，有两点需要注意采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行<em><strong>行、列的采样</strong></em>。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个（m &lt;&lt; M）。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。</p>
<p><em><strong>注意点：</strong></em></p>
<p><strong>设有N个样本，每个样本有M个features，决策树们其实都是随机地接受n个样本（对行随机取样）的m个feature（对列进行随机取样），每颗决策树的m个feature相同。每颗决策树其实都是对特定的数据进行学习归纳出分类方法，而随机取样可以保证有重复样本被不同决策树分类，这样就可以对不同决策树的分类能力做个评价。</strong></p>
<p><em><strong>随机森林 的过程：</strong></em></p>
<p>随机森林中的每一棵分类树为二叉树，其生成遵循<em><strong>自顶向下的递归分裂原则</strong></em>，即从根节点开始依次对训练集进行划分；在二叉树中，根节点包含全部训练数据， 按照节点纯度最小原则，分裂为左节点和右节点，它们分别包含训练数据的一个子集，按照同样的规则节点继续分裂，直到满足分支停止规则而停止生长。若节点n上的分类数据全部来自于同一类别，则此节点的纯度I(n)&#x3D;0，</p>
<p>纯度度量方法是Gini准则，即假设P(Xj)是节点n上属于Xj 类样本个数占训练。</p>
<p>具体实现过程如下：</p>
<p>（1）原始训练集为N，应用bootstrap法<strong>有放回地随机抽取k个新的自助样本集</strong>，并由此构建k棵分类树，每次未被抽到的样本组成了k个袋外数据；</p>
<p>（2）设有mall个变量，则在每一棵树的每个节点处随机抽取mtry个变量(mtry n mall)，然后在mtry中选择一个最具有分类能力的变量，变量分类的阈值通过检查每一个分类点确定；</p>
<p>（3）每棵树最大限度地生长, 不做任何修剪；</p>
<p>（4）将生成的多棵分类树组成随机森林，用随机森林分类器对新的数据进行判别与分类，分类结果按树分类器的投票多少而定。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u012102306/article/details/52228516">Random Forest (sklearn 参数详解)</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://shizhenqiang.github.io/2018/01/09/Apriori/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chelf">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chelf Blog">
      <meta itemprop="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Chelf Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/01/09/Apriori/" class="post-title-link" itemprop="url">Apriori</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-01-09 14:16:43" itemprop="dateCreated datePublished" datetime="2018-01-09T14:16:43+08:00">2018-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-25 11:45:38" itemprop="dateModified" datetime="2022-10-25T11:45:38+08:00">2022-10-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>看了很多博客，关于关联规则的介绍想做一个详细的汇总</p>
<h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a><strong>一、概念</strong></h1><p>表1 某超市的交易数据库</p>
<table>
<thead>
<tr>
<th>交易号TID</th>
<th>顾客购买的商品</th>
<th>交易号TID</th>
<th>顾客购买的商品</th>
</tr>
</thead>
<tbody><tr>
<td>T1</td>
<td>bread, cream, milk, tea</td>
<td>T6</td>
<td>bread, tea</td>
</tr>
<tr>
<td>T2</td>
<td>bread, cream, milk</td>
<td>T7</td>
<td>beer, milk, tea</td>
</tr>
<tr>
<td>T3</td>
<td>cake, milk</td>
<td>T8</td>
<td>bread, tea</td>
</tr>
<tr>
<td>T4</td>
<td>milk, tea</td>
<td>T9</td>
<td>bread, cream, milk, tea</td>
</tr>
<tr>
<td>T5</td>
<td>bread, cake, milk</td>
<td>T10</td>
<td>bread, milk, tea</td>
</tr>
</tbody></table>
<p><strong>定义一</strong>：设I&#x3D;{i1,i2,…,im}，是m个不同的项目的集合，每个ik称为一个<strong>项目</strong>。项目的集合I称为<strong>项集</strong>。其元素的个数称为项集的长度，长度为k的项集称为k-项集。引例中每个商品就是一个项目，项集为I&#x3D;{bread, beer, cake,cream, milk, tea}，I的长度为6。</p>
<p><strong>定义二</strong>：每笔<strong>交易</strong>T是项集I的一个子集。对应每一个交易有一个唯一标识交易号，记作TID。交易全体构成了<strong>交易数据库</strong>D，|D|等于D中交易的个数。引例中包含10笔交易，因此|D|&#x3D;10。</p>
<p><strong>定义三</strong>：对于项集X，设定count(X⊆T)为交易集D中包含X的交易的数量，则项集X的<strong>支持度</strong>为：</p>
<p>support(X)&#x3D;count(X⊆T)&#x2F;|D|</p>
<p>引例中X&#x3D;{bread, milk}出现在T1，T2，T5，T9和T10中，所以支持度为0.5。</p>
<p><strong>定义四</strong>：<strong>最小支持度</strong>是项集的最小支持阀值，记为SUPmin，代表了用户关心的关联规则的最低重要性。<strong>支持度不小于SUPmin 的项集称为频繁集</strong>，长度为k的频繁集称为k-频繁集。如果设定SUPmin为0.3，引例中{bread, milk}的支持度是0.5，所以是2-频繁集。</p>
<p><strong>定义五</strong>：<strong>关联规则</strong>是一个蕴含式：</p>
<p>R：X⇒Y</p>
<p>其中X⊂I，Y⊂I，并且X∩Y&#x3D;⌀。表示项集X在某一交易中出现，则导致Y以某一概率也会出现。用户关心的关联规则，可以用两个标准来衡量：支持度和可信度。</p>
<p><strong>定义六</strong>：关联规则R的<strong>支持度</strong>是交易集同时包含X和Y的交易数与|D|之比。即：</p>
<p>support(X⇒Y)&#x3D;count(X⋃Y)&#x2F;|D|</p>
<p>支持度反映了X、Y同时出现的概率。关联规则的支持度等于频繁集的支持度。 </p>
<p><strong>定义七</strong>：对于关联规则R，<strong>可信度</strong>是指包含X和Y的交易数与包含X的交易数之比。即：</p>
<p>confidence(X⇒Y)&#x3D;support(X⇒Y)&#x2F;support(X)</p>
<p>可信度反映了如果交易中包含X，则交易包含Y的概率。一般来说，只有支持度和可信度较高的关联规则才是用户感兴趣的。</p>
<p><strong>定义八</strong>：设定关联规则的最小支持度和最小可信度为SUPmin和CONFmin。规则R的支持度和可信度均不小于SUPmin和CONFmin ，则称为<strong>强关联规则</strong>。关联规则挖掘的目的就是找出强关联规则，从而指导商家的决策。</p>
<p>这八个定义包含了关联规则相关的几个重要基本概念，关联规则挖掘主要有两个问题：</p>
<ol>
<li>找出交易数据库中所有大于或等于用户指定的最小支持度的频繁项集。</li>
<li>利用频繁项集生成所需要的关联规则，根据用户设定的最小可信度筛选出强关联规则。</li>
</ol>
<p>目前研究人员主要针对第一个问题进行研究，找出频繁集是比较困难的，而有了频繁集再生成强关联规则就相对容易了。</p>
<h1 id="二、理论基础"><a href="#二、理论基础" class="headerlink" title="二、理论基础"></a>二、理论基础</h1><p>首先来看一个频繁集的性质。</p>
<p>定理：<strong>如果项目集X是频繁集，那么它的非空子集都是频繁集</strong>。</p>
<p>根据定理，已知一个k-频繁集的项集X，X的所有k-1阶子集都肯定是频繁集，也就肯定可以找到两个k-1频繁集的项集，它们只有一项不同，且连接后等于X。这证明了通过连接k-1频繁集产生的k-候选集覆盖了k-频繁集。同时，如果k-候选集中的项集Y，包含有某个k-1阶子集不属于k-1频繁集，那么Y就不可能是频繁集，应该从候选集中裁剪掉。Apriori算法就是利用了频繁集的这个性质。</p>
<h1 id="三、算法步骤："><a href="#三、算法步骤：" class="headerlink" title="三、算法步骤："></a><strong>三、算法步骤：</strong></h1><p>首先是测试数据：</p>
<table>
<thead>
<tr>
<th>交易ID</th>
<th><strong>商品<strong><strong>ID</strong></strong>列表</strong></th>
</tr>
</thead>
<tbody><tr>
<td>T100</td>
<td>I1，I2，I5</td>
</tr>
<tr>
<td>T200</td>
<td>I2，I4</td>
</tr>
<tr>
<td>T300</td>
<td>I2，I3</td>
</tr>
<tr>
<td>T400</td>
<td>I1，I2，I4</td>
</tr>
<tr>
<td>T500</td>
<td>I1，I3</td>
</tr>
<tr>
<td>T600</td>
<td>I2，I3</td>
</tr>
<tr>
<td>T700</td>
<td>I1，I3</td>
</tr>
<tr>
<td>T800</td>
<td>I1，I2，I3，I5</td>
</tr>
<tr>
<td>T900</td>
<td>I1，I2，I3</td>
</tr>
</tbody></table>
<p>算法的步骤图：</p>
<img src="/2018/01/09/Apriori/529698-20180109140825019-1205577597.png" class title="apriori asset_img"> 

<p>可以看到，第三轮的候选集发生了明显的缩小，这是为什么呢？</p>
<p>请注意取候选集的两个条件：</p>
<p>1.两个K项集能够连接的两个条件是，它们有K-1项是相同的。所以，（I2，I4）和（I3，I5）这种是不能够进行连接的。缩小了候选集。</p>
<p>2.如果一个项集是频繁集，那么它不存在不是子集的频繁集。比如（I1，I2）和（I1，I4）得到（I1，I2，I4），而（I1，I2，I4）存在子集（I1，I4）不是频繁集。缩小了候选集。</p>
<p>第三轮得到的2个候选集，正好支持度等于最小支持度。所以，都算入频繁集。</p>
<p>这时再看第四轮的候选集与频繁集结果为空</p>
<p>可以看到，候选集和频繁集居然为空了！因为通过第三轮得到的频繁集自连接得到{I1，I2，I3，I5}，它拥有子集{I2,I3,I5}，而{I2,I3,I5}不是频繁集，不满足：频繁集的子集也是频繁集这一条件，所以被剪枝剪掉了。所以整个算法终止，取最后一次计算得到的频繁集作为最终的频繁集结果：</p>
<p>也就是：[‘I1,I2,I3’, ‘I1,I2,I5’]</p>
<p><strong>四、代码：</strong>                                   </p>
<p>编写Python代码实现Apriori算法。代码需要注意如下两点：</p>
<ul>
<li>由于Apriori算法假定项集中的项是按字典序排序的，而集合本身是无序的，所以我们在必要时需要进行set和list的转换；</li>
<li>由于要使用字典（support_data）记录项集的支持度，需要用项集作为key，而可变集合无法作为字典的key，因此在合适时机应将项集转为固定集合frozenset。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">local_data</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">    dt = pd.read_excel(file_path)</span><br><span class="line">    data = dt[<span class="string">&#x27;con&#x27;</span>]</span><br><span class="line">    locdata = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        locdata.append(<span class="built_in">str</span>(i).split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># print(locdata)  # change to [[1,2,3],[1,2,3]]</span></span><br><span class="line">    length = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> locdata:</span><br><span class="line">        length.append(<span class="built_in">len</span>(i))  <span class="comment"># 计算长度并存储</span></span><br><span class="line">   <span class="comment"># print(length)</span></span><br><span class="line">    ki = length[length.index(<span class="built_in">max</span>(length))]</span><br><span class="line">   <span class="comment"># print(length[length.index(max(length))])  # length.index(max(length)读取最大值的位置，然后再定位取出最大值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> locdata,ki</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_C1</span>(<span class="params">data_set</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Create frequent candidate 1-itemset C1 by scaning data set.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_set: A list of transactions. Each transaction contains several items.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        C1: A set which contains all frequent candidate 1-itemsets</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    C1 = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> data_set:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> t:</span><br><span class="line">            item_set = <span class="built_in">frozenset</span>([item])</span><br><span class="line">            C1.add(item_set)</span><br><span class="line">    <span class="keyword">return</span> C1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_apriori</span>(<span class="params">Ck_item, Lksub1</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Judge whether a frequent candidate k-itemset satisfy Apriori property.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Ck_item: a frequent candidate k-itemset in Ck which contains all frequent</span></span><br><span class="line"><span class="string">                 candidate k-itemsets.</span></span><br><span class="line"><span class="string">        Lksub1: Lk-1, a set which contains all frequent candidate (k-1)-itemsets.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        True: satisfying Apriori property.</span></span><br><span class="line"><span class="string">        False: Not satisfying Apriori property.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> Ck_item:</span><br><span class="line">        sub_Ck = Ck_item - <span class="built_in">frozenset</span>([item])</span><br><span class="line">        <span class="keyword">if</span> sub_Ck <span class="keyword">not</span> <span class="keyword">in</span> Lksub1:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_Ck</span>(<span class="params">Lksub1, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Create Ck, a set which contains all all frequent candidate k-itemsets</span></span><br><span class="line"><span class="string">    by Lk-1&#x27;s own connection operation.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Lksub1: Lk-1, a set which contains all frequent candidate (k-1)-itemsets.</span></span><br><span class="line"><span class="string">        k: the item number of a frequent itemset.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        Ck: a set which contains all all frequent candidate k-itemsets.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Ck = <span class="built_in">set</span>()</span><br><span class="line">    len_Lksub1 = <span class="built_in">len</span>(Lksub1)</span><br><span class="line">    list_Lksub1 = <span class="built_in">list</span>(Lksub1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_Lksub1):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, len_Lksub1):</span><br><span class="line">            l1 = <span class="built_in">list</span>(list_Lksub1[i])</span><br><span class="line">            l2 = <span class="built_in">list</span>(list_Lksub1[j])</span><br><span class="line">            l1.sort()</span><br><span class="line">            l2.sort()</span><br><span class="line">            <span class="keyword">if</span> l1[<span class="number">0</span>:k-<span class="number">2</span>] == l2[<span class="number">0</span>:k-<span class="number">2</span>]:</span><br><span class="line">                Ck_item = list_Lksub1[i] | list_Lksub1[j]</span><br><span class="line">                <span class="comment"># pruning</span></span><br><span class="line">                <span class="keyword">if</span> is_apriori(Ck_item, Lksub1):</span><br><span class="line">                    Ck.add(Ck_item)</span><br><span class="line">    <span class="keyword">return</span> Ck</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_Lk_by_Ck</span>(<span class="params">data_set, Ck, min_support, support_data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate Lk by executing a delete policy from Ck.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_set: A list of transactions. Each transaction contains several items.</span></span><br><span class="line"><span class="string">        Ck: A set which contains all all frequent candidate k-itemsets.</span></span><br><span class="line"><span class="string">        min_support: The minimum support.</span></span><br><span class="line"><span class="string">        support_data: A dictionary. The key is frequent itemset and the value is support.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Lk: A set which contains all all frequent k-itemsets.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Lk = <span class="built_in">set</span>()</span><br><span class="line">    item_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> data_set:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="keyword">if</span> item.issubset(t):</span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_count:</span><br><span class="line">                    item_count[item] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    item_count[item] += <span class="number">1</span></span><br><span class="line">    t_num = <span class="built_in">float</span>(<span class="built_in">len</span>(data_set))</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_count:</span><br><span class="line">        <span class="keyword">if</span> (item_count[item] / t_num) &gt;= min_support:</span><br><span class="line">            Lk.add(item)</span><br><span class="line">            support_data[item] = item_count[item] / t_num</span><br><span class="line">    <span class="keyword">return</span> Lk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_L</span>(<span class="params">data_set, k, min_support</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate all frequent itemsets.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_set: A list of transactions. Each transaction contains several items.</span></span><br><span class="line"><span class="string">        k: Maximum number of items for all frequent itemsets.</span></span><br><span class="line"><span class="string">        min_support: The minimum support.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        L: The list of Lk.</span></span><br><span class="line"><span class="string">        support_data: A dictionary. The key is frequent itemset and the value is support.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    support_data = &#123;&#125;</span><br><span class="line">    C1 = create_C1(data_set)</span><br><span class="line">    L1 = generate_Lk_by_Ck(data_set, C1, min_support, support_data)</span><br><span class="line">    Lksub1 = L1.copy()</span><br><span class="line">    L = []</span><br><span class="line">    L.append(Lksub1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, k+<span class="number">1</span>):</span><br><span class="line">        Ci = create_Ck(Lksub1, i)</span><br><span class="line">        Li = generate_Lk_by_Ck(data_set, Ci, min_support, support_data)</span><br><span class="line">        Lksub1 = Li.copy()</span><br><span class="line">        L.append(Lksub1)</span><br><span class="line">    <span class="keyword">return</span> L, support_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_big_rules</span>(<span class="params">L, support_data, min_conf</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate big rules from frequent itemsets.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        L: The list of Lk.</span></span><br><span class="line"><span class="string">        support_data: A dictionary. The key is frequent itemset and the value is support.</span></span><br><span class="line"><span class="string">        min_conf: Minimal confidence.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        big_rule_list: A list which contains all big rules. Each big rule is represented</span></span><br><span class="line"><span class="string">                       as a 3-tuple.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    big_rule_list = []</span><br><span class="line">    sub_set_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(L)):</span><br><span class="line">        <span class="keyword">for</span> freq_set <span class="keyword">in</span> L[i]:</span><br><span class="line">            <span class="keyword">for</span> sub_set <span class="keyword">in</span> sub_set_list:</span><br><span class="line">                <span class="keyword">if</span> sub_set.issubset(freq_set):</span><br><span class="line">                    conf = support_data[freq_set] / support_data[freq_set - sub_set]</span><br><span class="line">                    big_rule = (freq_set - sub_set, sub_set, conf)</span><br><span class="line">                    <span class="keyword">if</span> conf &gt;= min_conf <span class="keyword">and</span> big_rule <span class="keyword">not</span> <span class="keyword">in</span> big_rule_list:</span><br><span class="line">                        <span class="comment"># print freq_set-sub_set, &quot; =&gt; &quot;, sub_set, &quot;conf: &quot;, conf</span></span><br><span class="line">                        big_rule_list.append(big_rule)</span><br><span class="line">            sub_set_list.append(freq_set)</span><br><span class="line">    <span class="keyword">return</span> big_rule_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Test</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    file_path = <span class="string">&quot;test_aa.xlsx&quot;</span></span><br><span class="line">  </span><br><span class="line">    data_set,k = local_data(file_path)</span><br><span class="line">    L, support_data = generate_L(data_set, k, min_support=<span class="number">0.2</span>)</span><br><span class="line">    big_rules_list = generate_big_rules(L, support_data, min_conf=<span class="number">0.4</span>)</span><br><span class="line">    <span class="built_in">print</span>(L)</span><br><span class="line">    <span class="keyword">for</span> Lk <span class="keyword">in</span> L:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>(Lk)) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;frequent &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="built_in">list</span>(Lk)[<span class="number">0</span>])) + <span class="string">&quot;-itemsets\t\tsupport&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">        <span class="keyword">for</span> freq_set <span class="keyword">in</span> Lk:</span><br><span class="line">            <span class="built_in">print</span>(freq_set, support_data[freq_set])</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Big Rules&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> big_rules_list:</span><br><span class="line">        <span class="built_in">print</span>(item[<span class="number">0</span>], <span class="string">&quot;=&gt;&quot;</span>, item[<span class="number">1</span>], <span class="string">&quot;conf: &quot;</span>, item[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<p> 文件格式：</p>
<p>test_aa.xlsx</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">name</span>    con</span><br><span class="line"><span class="attribute">T1</span>     <span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="attribute">T2</span>     <span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span></span><br><span class="line"><span class="attribute">T3</span>     <span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="attribute">T5</span>     <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line"><span class="attribute">T6</span>     <span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="attribute">T7</span>     <span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span></span><br><span class="line"><span class="attribute">T8</span>     <span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="attribute">T9</span>     <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line"><span class="attribute">T10</span>    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br></pre></td></tr></table></figure>

<p><strong>参考相关博客：</strong> 　　　　　　　　　　　　　　　　　　　</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/rongyongfeikai2/article/details/40457827">http://blog.csdn.net/rongyongfeikai2/article/details/40457827</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/opennaive/article/details/7051460">http://blog.csdn.net/opennaive/article/details/7051460</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/opennaive/article/details/7047823">http://blog.csdn.net/opennaive/article/details/7047823</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/androidlushangderen/article/details/43059211">http://blog.csdn.net/androidlushangderen/article/details/43059211</a></p>
<p>代码参考</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/qq_32126633/article/details/78351726">http://blog.csdn.net/qq_32126633/article/details/78351726</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://shizhenqiang.github.io/2017/07/10/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chelf">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chelf Blog">
      <meta itemprop="description" content="We used to look up at the sky and wonder at our place in the stars, now we just look down and worry about our place in the dirt.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Chelf Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/07/10/GBDT/" class="post-title-link" itemprop="url">GBDT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-07-10 16:22:54" itemprop="dateCreated datePublished" datetime="2017-07-10T16:22:54+08:00">2017-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-13 17:56:12" itemprop="dateModified" datetime="2022-10-13T17:56:12+08:00">2022-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="http://blog.csdn.net/yangtrees/article/details/7506052">机器学习——Gradient Boost Decision Tree(&amp;Treelink)</a></p>
<p><a target="_blank" rel="noopener" href="http://wenku.baidu.com/link?url=dmoD5irL8vAV5NnBmoz69M4J_tm-EZg8yGIcfEkTp_ApblW52F6LEv9GdJzusCCaTafPX5HxDObAtwJH77NmxOSp106W-kHCffX8KwC_YG7">greedy_function_approximation_a_gradient_boosting_machine</a></p>
<p><a target="_blank" rel="noopener" href="http://www.jianshu.com/p/005a4e6ac775">简书上很好的一个关于GBDT的例子</a></p>
<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><h2 id="一、Regression-Decision-Tree"><a href="#一、Regression-Decision-Tree" class="headerlink" title="一、Regression Decision Tree"></a>一、Regression Decision Tree</h2><p>​    回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但<strong>衡量最好的标准不再是最大熵，而是最小化平方误差</strong>。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的<em><strong>平均</strong></em>年龄做为该叶子节点的预测年龄。</p>
<h2 id="二、Boosting-Decision-Tree"><a href="#二、Boosting-Decision-Tree" class="headerlink" title="二、Boosting Decision Tree"></a>二、Boosting Decision Tree</h2><p>​    提升树是迭代多颗回归树来共同决策。每一棵决策树学习的是之前所有树的结论和残差(采用平方误差损失函数时)，拟合得到一个当前的残差回归树。残差的意义：<strong>残差 &#x3D; 真实值 - 预测值</strong>；提升树，即整个迭代过程生成的回归树的累加。预测值等于所有树值的累加。</p>
<img src="/2017/07/10/GBDT/1241" class title="model complex asset_img">



<p>​                                                                                                           提升树算法</p>
<h2 id="三、Gradient-Boosting-Decision-Tree"><a href="#三、Gradient-Boosting-Decision-Tree" class="headerlink" title="三、Gradient Boosting Decision Tree"></a>三、Gradient Boosting Decision Tree</h2><p>提升树利用加法模型和前向分类算法实现学习的优化过程。当损失函数平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。</p>
<p>但对于一般的损失函数，往往每一步优化没那么容易，如上图中的绝对值损失函数和Huber损失函数。针对这一问题，Freidman提出了梯度提升算法： 利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。</p>
<img src="/2017/07/10/GBDT/1240" class title="model complex asset_img">

<p>​    当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。<br>  当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chelf</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
